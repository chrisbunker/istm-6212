{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - Word Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A. Characters in Little Women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-09-23 13:12:34--  https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/women.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.20.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.20.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1053440 (1.0M) [text/plain]\n",
      "Saving to: ‘women.txt’\n",
      "\n",
      "women.txt           100%[=====================>]   1.00M  --.-KB/s   in 0.05s  \n",
      "\n",
      "2016-09-23 13:12:35 (21.9 MB/s) - ‘women.txt’ saved [1053440/1053440]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/women.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    652     652    2608\r\n"
     ]
    }
   ],
   "source": [
    "!grep -o \"Jo \" women.txt | wc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    227     227    1362\r\n"
     ]
    }
   ],
   "source": [
    "!grep -o \"Beth \" women.txt | wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    351     351    1755\r\n"
     ]
    }
   ],
   "source": [
    "!grep -o \"Meg \" women.txt | wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    333     333    1665\r\n"
     ]
    }
   ],
   "source": [
    "!grep -o \"Amy \" women.txt | wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the text Little Women, Jo is mentioned 652 times, Beth is mentioned 227 times, Meg is mentioned 351 times, and Amy is mentioned 333 times.\n",
    "\n",
    "I used the \"grep -o\" command which counts each time the word shows up in each line. I did not use the command that is given within the project file because it kept failing on me and not outputting the right information. I found that I got what I think is the correct answer by just utilizing the first part of the command and then accounting for white space after the words to ensure it captures only those words. I do not account for white space before the words because if they are at the beginning of a paragraph I do not believe that there would be white space before it in the line.\n",
    "\n",
    "At first Jo printed out over 1000 times and I was not sure why it was so high. I then realized that I didnt account for the fact that it would also count words that began with Jo, so I had to encapsulate the search with spaces before and after. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B. Juliet and Romeo in Romeo and Julie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-09-23 13:12:37--  https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/romeo.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.20.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.20.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 178983 (175K) [text/plain]\n",
      "Saving to: ‘romeo.txt’\n",
      "\n",
      "romeo.txt           100%[=====================>] 174.79K  --.-KB/s   in 0.02s  \n",
      "\n",
      "2016-09-23 13:12:37 (8.07 MB/s) - ‘romeo.txt’ saved [178983/178983]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/romeo.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    312    2408   14353\r\n"
     ]
    }
   ],
   "source": [
    "!grep Rom. romeo.txt | wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    189    1511    8915\r\n"
     ]
    }
   ],
   "source": [
    "!grep Jul. romeo.txt | wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this is a play and we want to know the amount of times Romeo and Juliet have speaking lines, we have to see how the play script denotes when they each speak. In this text, they do so using the first 3 letters of their name and a period, so instead of seeing where their names are in the text, we get a word count for Rom. and Jul.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - Capital Bikeshare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-09-23 13:12:41--  https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/2016q1.csv.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.20.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.20.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10643003 (10M) [application/octet-stream]\n",
      "Saving to: ‘2016q1.csv.zip’\n",
      "\n",
      "2016q1.csv.zip      100%[=====================>]  10.15M  65.5MB/s   in 0.2s   \n",
      "\n",
      "2016-09-23 13:12:41 (65.5 MB/s) - ‘2016q1.csv.zip’ saved [10643003/10643003]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/2016q1.csv.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  2016q1.csv.zip\n",
      "  inflating: 2016q1.csv              \n"
     ]
    }
   ],
   "source": [
    "!unzip 2016q1.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5514 17th & Corcoran St NW\r\n",
      "   5649 Eastern Market Metro / Pennsylvania Ave & 7th St SE\r\n",
      "   6491 New Hampshire Ave & T St NW\r\n",
      "   6568 14th & V St NW\r\n",
      "   7401 15th & P St NW\r\n",
      "   7479 Thomas Circle\r\n",
      "   8138 Jefferson Dr & 14th St SW\r\n",
      "   9388 Lincoln Memorial\r\n",
      "   9560 Massachusetts Ave & Dupont Circle NW\r\n",
      "  13120 Columbus Circle / Union Station\r\n"
     ]
    }
   ],
   "source": [
    "!csvcut -c5 2016q1.csv | sort | uniq -c | sort -n | tail -10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5651 17th & Corcoran St NW\r\n",
      "   5761 5th & K St NW\r\n",
      "   6245 New Hampshire Ave & T St NW\r\n",
      "   6997 Thomas Circle\r\n",
      "   7267 14th & V St NW\r\n",
      "   8092 15th & P St NW\r\n",
      "   8975 Jefferson Dr & 14th St SW\r\n",
      "   9419 Lincoln Memorial\r\n",
      "  11183 Massachusetts Ave & Dupont Circle NW\r\n",
      "  13880 Columbus Circle / Union Station\r\n"
     ]
    }
   ],
   "source": [
    "!csvcut -c7 2016q1.csv | sort | uniq -c | sort -n | tail -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1. Start station\r\n",
      "\t<class 'str'>\r\n",
      "\tNulls: False\r\n",
      "\tUnique values: 370\r\n",
      "\t5 most frequent values:\r\n",
      "\t\tColumbus Circle / Union Station:\t13120\r\n",
      "\t\tMassachusetts Ave & Dupont Circle NW:\t9560\r\n",
      "\t\tLincoln Memorial:\t9388\r\n",
      "\t\tJefferson Dr & 14th St SW:\t8138\r\n",
      "\t\tThomas Circle:\t7479\r\n",
      "\tMax length: 64\r\n",
      "\r\n",
      "Row count: 552399\r\n"
     ]
    }
   ],
   "source": [
    "!csvcut -c5 2016q1.csv | csvstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1. End station\r\n",
      "\t<class 'str'>\r\n",
      "\tNulls: False\r\n",
      "\tUnique values: 370\r\n",
      "\t5 most frequent values:\r\n",
      "\t\tColumbus Circle / Union Station:\t13880\r\n",
      "\t\tMassachusetts Ave & Dupont Circle NW:\t11183\r\n",
      "\t\tLincoln Memorial:\t9419\r\n",
      "\t\tJefferson Dr & 14th St SW:\t8975\r\n",
      "\t\t15th & P St NW:\t8092\r\n",
      "\tMax length: 64\r\n",
      "\r\n",
      "Row count: 552399\r\n"
     ]
    }
   ],
   "source": [
    "!csvcut -c7 2016q1.csv | csvstat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Answer\n",
    "\n",
    "The 10 Capital Bikeshare stations that were the most popular departing stations in Q1 2016 were:\n",
    "    1. Columbus Circle / Union Station  - 13120\n",
    "    2. Massachusetts Ave & Dupont Circle NW - 9560\n",
    "    3. Lincoln Memorial - 9388\n",
    "    4. Jefferson Dr & 14th St SW - 8138\n",
    "    5. Thomas Circle - 7479\n",
    "    6. 15th & P St NW - 7401\n",
    "    7. 14th & V St NW - 6568 \n",
    "    8. New Hampshire Ave & T St NW - 6491\n",
    "    9. Eastern Market Metro / Pennsylvania Ave & 7th St SE - 5649\n",
    "    10. 17th & Corcoran St NW 5514\n",
    " \n",
    "The 10 stations that were the most popular destination stations in Q1 2016 were:    \n",
    "    1. Columbus Circle / Union Station  - 13880\n",
    "    2. Massachusetts Ave & Dupont Circle NW - 11183\n",
    "    3. Lincoln Memorial - 9419\n",
    "    4. Jefferson Dr & 14th St SW - 8975\n",
    "    5. 15th & P St NW - 8092\n",
    "    6. 14th & V St NW - 7267\n",
    "    7. Thomas Circle - 6997 \n",
    "    8. New Hampshire Ave & T St NW - 6245\n",
    "    9. 17th & Corcoran St NW - 5761\n",
    "    10. 5th & K St NW - 5651\n",
    "\n",
    "By utlizing the sort function I placed each field in the column next to its identical fields, then counted the values by filtering the fields to count the number of unique fields. In order to get the proper values outputted, I needed to sort the column by number not character and then get the last 10 because it sorts from least to greatest. I also used the statistical filter csvstat just to double check that my answers were correct for both the departing stations and destination stations because it gives the top 5 most frequent values. The majority of the most popular departing stations are also the most popular destination stations, this makes sense because they if you head to one of these places, chances are you are also going to depart from this station as well to leave. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|------------------------------------------+----------|\r\n",
      "|  column1                                 | column2  |\r\n",
      "|------------------------------------------+----------|\r\n",
      "|       15 Columbus Circle / Union Station | W21076   |\r\n",
      "|       15 Columbus Circle / Union Station | W21450   |\r\n",
      "|       15 Columbus Circle / Union Station | W22080   |\r\n",
      "|       16 Columbus Circle / Union Station | W00714   |\r\n",
      "|       16 Columbus Circle / Union Station | W20540   |\r\n",
      "|       16 Columbus Circle / Union Station | W21239   |\r\n",
      "|       16 Columbus Circle / Union Station | W21538   |\r\n",
      "|       16 Columbus Circle / Union Station | W21641   |\r\n",
      "|       16 Columbus Circle / Union Station | W21867   |\r\n",
      "|       17 Columbus Circle / Union Station | W22227   |\r\n",
      "|------------------------------------------+----------|\r\n"
     ]
    }
   ],
   "source": [
    "!csvcut -c5,8 2016q1.csv | csvgrep -c1 -m \"Columbus Circle\" | sort | uniq -c | sort -n | tail -10 | csvlook -H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|------------------------------------------+----------|\r\n",
      "|  column1                                 | column2  |\r\n",
      "|------------------------------------------+----------|\r\n",
      "|       15 Columbus Circle / Union Station | W21867   |\r\n",
      "|       15 Columbus Circle / Union Station | W21997   |\r\n",
      "|       16 Columbus Circle / Union Station | W00714   |\r\n",
      "|       16 Columbus Circle / Union Station | W20425   |\r\n",
      "|       16 Columbus Circle / Union Station | W21076   |\r\n",
      "|       16 Columbus Circle / Union Station | W21239   |\r\n",
      "|       16 Columbus Circle / Union Station | W22080   |\r\n",
      "|       16 Columbus Circle / Union Station | W22099   |\r\n",
      "|       17 Columbus Circle / Union Station | W22227   |\r\n",
      "|       18 Columbus Circle / Union Station | W00485   |\r\n",
      "|------------------------------------------+----------|\r\n"
     ]
    }
   ],
   "source": [
    "!csvcut -c7,8 2016q1.csv | csvgrep -c1 -m \"Columbus Circle\" | sort | uniq -c | sort -n | tail -10 | csvlook -H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Part B: Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The 10 most popular bike for departing at Columbus Circle were:\n",
    "    1. W22227\n",
    "    2. W21867\n",
    "    3. W21641\n",
    "    4. W21538\n",
    "    5. W21239\n",
    "    6. W20540\n",
    "    7. W00714\n",
    "    8. W22080\n",
    "    9. W21450\n",
    "    10. W21076\n",
    "The 10 most popular bike for arriving at Columbus Circle were:\n",
    "    1. W00485\n",
    "    2. W22227\n",
    "    3. W22099\n",
    "    4. W22080\n",
    "    5. W21239\n",
    "    6. W21076\n",
    "    7. W20425\n",
    "    8. W00714 \n",
    "    9. W21997\n",
    "    10. W21867\n",
    "\n",
    "I grabbed the most popular departing station and destination station and only searched for bikes the corresponding bike number column respectively. Next I sorted by the columns we made in the cut ( and counted them. Again I took the tail so that I have the 10 most popular. I created default headers again to make the output more readable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 - Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   8155 and\r\n",
      "   7689 the\r\n",
      "   5152 to\r\n",
      "   3523 of\r\n",
      "   3245 her\r\n",
      "   2774 it\r\n",
      "   2503 in\r\n",
      "   2447 you\r\n",
      "   2343 she\r\n",
      "   2233 for\r\n",
      "   2033 was\r\n",
      "   1978 as\r\n",
      "   1937 that\r\n",
      "   1854 with\r\n",
      "   1598 he\r\n",
      "   1469 but\r\n",
      "   1362 jo\r\n",
      "   1135 so\r\n",
      "   1118 his\r\n",
      "   1067 at\r\n",
      "   1063 had\r\n",
      "   1014 be\r\n",
      "    976 on\r\n",
      "    942 not\r\n",
      "    916 if\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!cat women.txt | python onewordperline.py | python lowercase.py | sort | uniq -c | sort -rn | head -25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use two python files that split the words in separate lines and make all characters lowercase respectively. I used Idle, a python GUI, to write the formulas, I have used it in the past and it is what I am most comfortable using to write python code. We then use the same filters as before to count the total unique words and take the last 25 to find the stop words. There are 24 stop words that I will remove (the 25th word that was printed out was jo, which is not a stop word). The top 24 stop words I will remove next are:\n",
    "\n",
    "    1. and\n",
    "    2. the\n",
    "    3. to\n",
    "    4. of\n",
    "    5. her\n",
    "    6. it\n",
    "    7. in\n",
    "    8. you\n",
    "    9. she\n",
    "    10. for\n",
    "    11. was\n",
    "    12. as\n",
    "    13. that\n",
    "    14. with\n",
    "    15. he\n",
    "    16. but\n",
    "    17. so\n",
    "    18. his\n",
    "    19. at\n",
    "    20. had\n",
    "    21. be\n",
    "    22. on\n",
    "    23. not\n",
    "    24. if\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1362 jo\r\n",
      "    881 all\r\n",
      "    843 my\r\n",
      "    827 said\r\n",
      "    821 is\r\n",
      "    782 him\r\n",
      "    755 me\r\n",
      "    730 little\r\n",
      "    725 one\r\n",
      "    719 they\r\n",
      "    717 have\r\n",
      "    709 when\r\n",
      "    708 do\r\n",
      "    686 meg\r\n",
      "    658 up\r\n",
      "    652 amy\r\n",
      "    632 by\r\n",
      "    619 which\r\n",
      "    604 we\r\n",
      "    598 laurie\r\n",
      "    591 like\r\n",
      "    550 don\r\n",
      "    544 no\r\n",
      "    517 were\r\n",
      "    513 or\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!cat women.txt| python onewordperline.py | python lowercase.py | python stopwords.py | sort | uniq -c | sort -rn | head -25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we add an additional filter to remove the stop words that we found in Part A. We then print out the new top 25 words to ensure that the stop words we wanted to take out are all removed. \n",
    "\n",
    "For this problem I could not use \"./file.py\" and I am not sure why, but after researching I found that by adding python before the file name and then within the file taking out the \">>>\" at the beginning and end, it read the files perfectly. It even gave me some syntax error help when it would not print out properly at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
